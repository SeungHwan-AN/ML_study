rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
# Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
# beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
# Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
# beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
# Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
# beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
# result
cat("Test: sum of beta < 1e-6 =", sum(result$beta_path[, result$steps]) < 1e-6, "\n")
# plot
par(mfrow = c(1,1))
beta_max = max(result$beta_path)
beta_min = min(result$beta_path)
sum_abs_beta = apply(abs(result$beta_path), 2, sum)
plot(sum_abs_beta, result$beta_path[1, ], ylim = c(beta_min, beta_max), type = 'l', col = 1, lwd = 2,
xlab = "sum(abs(beta))", ylab = "beta", main = 'BETA coefs PATH(Constrained LASSO)')
for(i in 2:p) points(sum_abs_beta, result$beta_path[i, ], type = 'l', col = i, lwd = 2)
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
# Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
# beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
# result
cat("Test: sum of beta < 1e-6 =", sum(result$beta_path[, result$steps]) < 1e-6, "\n")
# plot
par(mfrow = c(1,1))
beta_max = max(result$beta_path)
beta_min = min(result$beta_path)
sum_abs_beta = apply(abs(result$beta_path), 2, sum)
plot(sum_abs_beta, result$beta_path[1, ], ylim = c(beta_min, beta_max), type = 'l', col = 1, lwd = 2,
xlab = "sum(abs(beta))", ylab = "beta", main = 'BETA coefs PATH(Constrained LASSO)')
for(i in 2:p) points(sum_abs_beta, result$beta_path[i, ], type = 'l', col = i, lwd = 2)
result$beta_path
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
# Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
# beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
# result
cat("Test: sum of beta < 1e-6 =", sum(result$beta_path[, result$steps]) < 1e-6, "\n")
# plot
par(mfrow = c(1,1))
beta_max = max(result$beta_path)
beta_min = min(result$beta_path)
sum_abs_beta = apply(abs(result$beta_path), 2, sum)
plot(sum_abs_beta, result$beta_path[1, ], ylim = c(beta_min, beta_max), type = 'l', col = 1, lwd = 2,
xlab = "sum(abs(beta))", ylab = "beta", main = 'BETA coefs PATH(Constrained LASSO)')
for(i in 2:p) points(sum_abs_beta, result$beta_path[i, ], type = 'l', col = i, lwd = 2)
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
n = dim(X)[1]
p = dim(X)[2]
if(n < p) {
print("Adding a small ridge penalty (default is 1e-4) since n < p")
if(rho_ridge <= 0) {
print("ρridge must be positive, switching to default value (1e-4)")
rho_ridge = 1e-4
}
# create augmented data
y = rbind(y, matrix(rep(0, p), nrow = p))
X = rbind(X, sqrt(rho_ridge) * diag(rep(1, p)))
# record original number of observations
# n_orig = n
} else {
# make sure X is full column rank
qrfact_X = qr(X)
R = qr.R(qrfact_X)
rankX = sum(abs(diag(R)) > (abs(R[1,1]) * max(n,p) * (.Machine$double.eps) ^ 2))  # 4.930380657631324e-32 *****
if(rankX != p) {
print("Adding a small ridge penalty (default is 1e-4) since X is rank deficient")
if(rho_ridge <= 0) {
print("ρridge must be positive, switching to default value (1e-4)")
rho_ridge = 1e-4
}
# create augmented data
y = rbind(y, matrix(rep(0, p), nrow = p))
X = rbind(X, sqrt(rho_ridge) * diag(rep(1, p)))
}
}
# alrhocate variables arhong path
neq = dim(Aeq)[1]
nineq = dim(Aineq)[1]
maxiters = 5 * (p + nineq) # max number of path segments to consider
beta_path = matrix(rep(0, p * maxiters), nrow = p)
lambda_patheq = matrix(rep(0, neq * maxiters), nrow = neq) # dual variables for equality
mu_pathineq = matrix(rep(0, nineq * maxiters), nrow = nineq) # dual variables for inequality
rho_path = rep(0, maxiters) # tuning parameter
df_path = rep(Inf, maxiters) # degree of freedom
objval_path = rep(0, maxiters) # objective value
violation_path = rep(Inf, maxiters)
### initialization
# use LP to find ρmax
H = t(X) %*% X
# find the maximum ρ (starting value)
l = find_rho_max(X, y, Aeq, beq, Aineq, bineq, penidx)
rho_path[1] = l$rho_max
idx = l$ind_rho_max
penidx = rep(T, p)
# find the maximum ρ (starting value)
l = find_rho_max(X, y, Aeq, beq, Aineq, bineq, penidx)
rho_path[1] = l$rho_max
idx = l$ind_rho_max
# calculate at ρmax
l = constrsparsereg(X, y, Aeq, beq, Aineq, bineq,
rho = rho_path[1], penwt = penidx)
beta_path[, 1] = l$beta_value
objval_path[1] = l$problem.optval
problem = l$problem
result = l$result
for(i in 1:min(2, length(problem@constraints))) {
if(canonicalize(problem@constraints[[i]])[[2]][[1]]$class == "LinEqConstr") {
lambda_patheq[, 1] = result$getDualValue(problem@constraints[[i]]) # 이거 값 다름 - 왜 0 나오지?***** (부호변경)
} else if(canonicalize(problem@constraints[[i]])[[2]][[1]]$class == "LinLeqConstr") {
mu_pathineq[, 1] = result$getDualValue(problem@constraints[[i]])
}
}
#???
mu_pathineq[mu_pathineq < 0] = 0
setActive = (abs(beta_path[, 1]) > 1e-4) | (!penidx)
beta_path[!setActive, 1] = 0
residIneq = Aineq %*% beta_path[, 1] - bineq
setIneqBorder = residIneq == 0
nIneqBorder = sum(setIneqBorder != 0)
# initialize subgradient vector (stationarity condition)
resid = y - X %*% beta_path[, 1]
if(neq > 0 & nineq > 0) {
subgrad = t(X) %*% resid - t(Aeq) %*% lambda_patheq[ ,1] - t(Aineq) %*% mu_pathineq[, 1]
} else if(neq > 0 & nineq == 0) {
subgrad = t(X) %*% resid - t(Aeq) %*% lambda_patheq[ ,1]
} else if(neq == 0 & nineq > 0) {
subgrad = t(X) %*% resid - t(Aineq) %*% mu_pathineq[, 1]
}
# subgradient of beta
subgrad[setActive] = sign(beta_path[setActive, 1])
subgrad[!setActive] = subgrad[!setActive] / rho_path[1]
setActive[idx] = T
nActive = sum(setActive != 0)
# calculate degrees of freedom
# rankAeq = Matrix::rankMatrix(Aeq) ### Brian's comment: need to make it more efficient
rankAeq = ifelse(dim(Aeq)[1] > 0, Matrix::rankMatrix(Aeq), 0)
df_path[1] = nActive - rankAeq - nIneqBorder
# set initial violations counter to 0
violation_path[1] = 0
# calculate derivative for coefficients and multipliers
# construct matrix
activeCoeffs = which(setActive)
inactiveCoeffs = which(!setActive)
idxIneqBorder = which(setIneqBorder)
# 여기 계산 불안정 - 제약조건이 1개만(등호 or 부등호) 있을 때*****
M = cbind(H[activeCoeffs, activeCoeffs, drop=F], t(Aeq[, activeCoeffs, drop=F]),
t(Aineq[setIneqBorder, activeCoeffs, drop=F]))
M = rbind(M, matrix(rep(0, (neq + nIneqBorder) * dim(M)[2]), nrow = (neq + nIneqBorder)))
M[(nrow(M) - neq - nIneqBorder + 1):nrow(M), 1:nActive] = rbind(Aeq[, activeCoeffs, drop=F],
Aineq[idxIneqBorder, activeCoeffs, drop=F])
eigen(M)
solve(M, rbind(matrix(subgrad[setActive], ncol = 1), matrix(rep(0, neq + nIneqBorder), ncol = 1)))
solve(M, rbind(matrix(subgrad[setActive], ncol = 1), matrix(rep(0, neq + nIneqBorder), ncol = 1)))
eigen(M)
solve(M)
M
min(eigen(M))
min(eigen(M)$values)
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
# Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
# beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
# result
cat("Test: sum of beta < 1e-6 =", sum(result$beta_path[, result$steps]) < 1e-6, "\n")
# plot
par(mfrow = c(1,1))
beta_max = max(result$beta_path)
beta_min = min(result$beta_path)
sum_abs_beta = apply(abs(result$beta_path), 2, sum)
plot(sum_abs_beta, result$beta_path[1, ], ylim = c(beta_min, beta_max), type = 'l', col = 1, lwd = 2,
xlab = "sum(abs(beta))", ylab = "beta", main = 'BETA coefs PATH(Constrained LASSO)')
for(i in 2:p) points(sum_abs_beta, result$beta_path[i, ], type = 'l', col = i, lwd = 2)
rm(list = ls())
gc()
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/classo")
source("find_rho_max.R")
source("constrsparsereg.R")
source("classopath.R")
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#############################################################################
# Calculate the solution path of the constrained lasso problem that minimizes
# `0.5sumabs2(√obswt .* (y - X * β)) + ρ * sumabs(penwt .* β)`
# subject to linear constraints.
#############################################################################
### truth with sum constrant sum(beta) = 0
# β = zeros(p)
# β[1:round(Int, p / 4)] = 0
# β[(round(Int, p / 4) + 1):round(Int, p / 2)] = 1
# β[(round(Int, p / 2) + 1):round(Int, 3p / 4)] = 0
# β[(round(Int, 3p / 4) + 1):p] = -1
### generate data - normalize된 data 사용
X=as.matrix(read.csv("C:/Julia/classo/src/X1.csv", header = F)) # n * p
y=as.matrix(read.csv("C:/Julia/classo/src/y1.csv", header = F)) # n
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# default
# Aeq = matrix(0, nrow = 0, ncol = dim(X)[2])
# beq = rep(0, dim(Aeq)[1])
# use
Aeq = matrix(rep(1, p), nrow = 1) # 1×20 Array{Frhoat64,2}
beq = matrix(0, nrow = 1) # 1-element Array{Fsrhoat64,1}
# another example
# Aeq = matrix(1, nrow = 2, p); Aeq[1,11:20] = 0; Aeq[2,1:10] = 0
# beq = matrix(0, nrow = 2, 1)
### inequality constraints
# default
Aineq = matrix(0, nrow = 0, ncol = dim(X)[2])
bineq = rep(0, dim(Aineq)[1])
# use
# Aineq = -diag(rep(1, p))
# bineq = rep(0, p)
# penalty weight
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
# do it!
result = classopath(X, y, Aeq, beq, Aineq, bineq, penwt)
# result
cat("Test: sum of beta < 1e-6 =", sum(result$beta_path[, result$steps]) < 1e-6, "\n")
# plot
par(mfrow = c(1,1))
beta_max = max(result$beta_path)
beta_min = min(result$beta_path)
sum_abs_beta = apply(abs(result$beta_path), 2, sum)
plot(sum_abs_beta, result$beta_path[1, ], ylim = c(beta_min, beta_max), type = 'l', col = 1, lwd = 2,
xlab = "sum(abs(beta))", ylab = "beta", main = 'BETA coefs PATH(Constrained LASSO)')
for(i in 2:p) points(sum_abs_beta, result$beta_path[i, ], type = 'l', col = i, lwd = 2)
