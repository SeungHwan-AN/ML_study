penwt = rep(1, p) # 20-element Array{Frhoat64,1}
###
l = conlasso_eq(X, y, Aeq, beq, show_plotting=T)
l
k = 11
delta = Variable(1)
constraints = list(-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
<= (rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
>= -(rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
delta >= 0)
objective = Maximize(delta)
active_set
rm(list = ls())
gc()
#####################################
# JUST FOR ORDINARY REGRESSION LOSS #
#####################################
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/conlasso_eq/src")
source("conlasso_init.R")
source("conlasso_eq.R")
#
set.seed(520)
### setting --------------------------------------------------
# dimension
n = 100
p = 10
m = 5
# data
X = matrix(rnorm(n*p), nrow = n)
X.m = apply(X, 2, mean)
X.m = matrix(X.m, nrow = n, ncol = p, byrow = T)
X = X - X.m
true_b = rep(1, p)
y = X %*% true_b + rnorm(n)
y = y - mean(y)
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# Aeq = matrix(rnorm(m*p, 0, 1), nrow = m)
Aeq = matrix(runif(m*p, min = -1, max = 2), nrow = m)
beq = matrix(rep(0, m), nrow = m)
# penalty weight*****(NOT USED)
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
n = dim(X)[1]
p = dim(X)[2]
# variable definition
neq = dim(Aeq)[1]
maxiters = 5 * p # max number of path segments to consider
beta_path = matrix(rep(0, p * maxiters), nrow = p)
lambda_patheq = matrix(rep(0, neq * maxiters), nrow = neq) # dual variables for equality
rho_path = rep(0, maxiters) # tuning parameter
objval_path = rep(0, maxiters) # objective value
# violation_path = rep(Inf, maxiters)
### initialization --------------------------------------------------
H = t(X) %*% X
# find the maximum ρ and corresponding lambda (starting value)
l = path_init(X, y, Aeq, beq) # no inequality constraints
rho_path[1] = l$rho_max
lambda_patheq[, 1] = l$lambda_max
# subgradient for init_beta = 0
beta_path[, 1] = 0
resid = y - X %*% beta_path[, 1]
subgrad = (t(X) %*% resid - t(Aeq) %*% lambda_patheq[ ,1]) / rho_path[1]
# loss value
objval_path[1] = sum((y - X %*% beta_path[ ,1])^2) + rho_path[1] * sum(abs(beta_path[1]))
# initial active set
active_set = rep(F, p)
active_set[l$activeset] = T
num_active = sum(active_set)
### main path following algorithm -------------------------------------------
for(k in 2:10) {
### find direction
H = t(X) %*% X
M = rbind(cbind(H[active_set, active_set], t(Aeq[, active_set])),
cbind(Aeq[, active_set], matrix(rep(0, m*m), nrow = m)))
S = rbind(subgrad[active_set, , drop=F], matrix(rep(0, m), ncol = 1))
if(min(eigen(M)$values) > 0) {
b_l = solve(M, S)
} else {
b_l = MASS::ginv(M) %*% S
}
# derivative for beta and lambda for rho
delta_b = b_l[1:num_active, ,drop=F]
delta_l = b_l[(num_active+1):nrow(b_l), ,drop=F]
### find delta_rho
delta_rho_vec = c()
# 1. active -> inactive (== subgradient rule check)
nonzero_active_beta = which(active_set)[which(beta_path[active_set, k-1] != 0)]
# predictor have potential to shrink 0
# -> which beta coef and delta_beta have different direction
sub_mismatch = sign(beta_path[nonzero_active_beta, k-1]) * sign(delta_b[which(beta_path[active_set, k-1] != 0), ])
# there exists potential
if(any(sub_mismatch == 1)) {
delta_rho_sub_tmp = beta_path[nonzero_active_beta[which(sub_mismatch == 1)], k-1] /
delta_b[which(beta_path[active_set, k-1] != 0), ][which(sub_mismatch == 1)]
delta_rho = min(delta_rho_sub_tmp)
delta_rho_vec = c(delta_rho_vec, delta_rho)
# predictor shrink to zero
sub_viol = which(active_set)[which(beta_path[which(active_set), k-1] / delta_b == delta_rho)]
} else {
delta_rho_vec = c(delta_rho_vec, -1)
}
# 2. dual feasibility & KKT - stationarity condition
# for NOT active set
if(sum(active_set) != p) { # if all predictors are activated, do not run opt problem(all predictors are actived)
delta = Variable(1)
constraints = list(-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
<= (rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
>= -(rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
delta >= 0)
objective = Maximize(delta)
problem = Problem(objective, constraints)
result = solve(problem)
# find predictor which cannot satisfy KKT condition among NOT active set
# -> this must be actived
kkt_viol_to_active = c()
# NA for delta value : there exist some predictor which never satisfy KKT condition
if(is.na(result$getValue(delta))) {
delta_rho_vec = c(delta_rho_vec, -1)
# predictors which cannot satisfy KKT condition
kkt_viol_to_active1 = which(-t(X[, !active_set]) %*% (y - X[, active_set] %*% beta_path[active_set, k-1]) +
t(Aeq[, !active_set]) %*% lambda_patheq[, k-1] >
rho_path[k-1] * matrix(rep(1, sum(!active_set)), ncol = 1))
kkt_viol_to_active2 = which(-t(X[, !active_set]) %*% (y - X[, active_set] %*% beta_path[active_set, k-1]) +
t(Aeq[, !active_set]) %*% lambda_patheq[, k-1] <
- rho_path[k-1] * matrix(rep(1, sum(!active_set)), ncol = 1))
kkt_viol_to_active = sort(union(kkt_viol_to_active1, kkt_viol_to_active2)) # next added predictor
} else {
delta_rho_vec = c(delta_rho_vec, result$getValue(delta))
# predictor on boundary(for update active set) -> update for active_set
delta_rho_kkt_tmp = result$getValue(delta)
kkt_viol1 = which(abs(-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta_rho_kkt_tmp * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta_rho_kkt_tmp * delta_l) -
(rho_path[k-1] - delta_rho_kkt_tmp) * matrix(rep(1, sum(!active_set)), ncol = 1)) <= 1e-6)
kkt_viol2 = which(abs(-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta_rho_kkt_tmp * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta_rho_kkt_tmp * delta_l) +
(rho_path[k-1] - delta_rho_kkt_tmp) * matrix(rep(1, sum(!active_set)), ncol = 1)) <= 1e-6)
kkt_viol = sort(union(kkt_viol1, kkt_viol2)) # next added predictor
}
# all predictors are activated
} else {
delta_rho_vec = c(delta_rho_vec, -1)
}
### terminate checking (rho becomes 0)
if(any(delta_rho_vec > 0)) {
if(min(delta_rho_vec[delta_rho_vec > 0]) > rho_path[k-1]) {
delta_rho = rho_path[k-1]
rho_path[k] = rho_path[k-1] - delta_rho
beta_path[active_set, k] = beta_path[active_set, k-1, drop=F] - delta_rho * delta_b
objval_path[k] = sum((y - X %*% beta_path[ ,k])^2) + rho_path[k] * sum(abs(beta_path[k]))
lambda_patheq[, k] = lambda_patheq[, k-1] - delta_rho * delta_l
break
}
}
### choose delta_rho
# 1. KKT condition is only violated
if(delta_rho_vec[1] < 0 & delta_rho_vec[2] > 0) {
delta_rho = delta_rho_vec[2]
min_idx = 2
# 2. subgradient rule is only violated
} else if(delta_rho_vec[1] > 0 & delta_rho_vec[2] < 0){
delta_rho = delta_rho_vec[1]
min_idx = 1
# 3. choose first violated rule
} else if(delta_rho_vec[1] > 0 & delta_rho_vec[2] > 0){
delta_rho = min(delta_rho_vec)
min_idx = which.min(delta_rho_vec)
# 4. there is no candidates for delta_rho
} else if(delta_rho_vec[1] < 0 & delta_rho_vec[2] < 0){
# if we need to change active set (NOT terminate algorithm)
if(length(kkt_viol_to_active) > 0) {
rho_path[k] = rho_path[k-1]
beta_path[active_set, k] = beta_path[active_set, k-1, drop=F]
objval_path[k] = sum((y - X %*% beta_path[ ,k])^2) + rho_path[k] * sum(abs(beta_path[k]))
lambda_patheq[, k] = lambda_patheq[, k-1]
active_set[which(!active_set)[kkt_viol_to_active]] = T
num_active = sum(active_set)
next
} else {
### terminate algorithm (no event is upcoming)
delta_rho = rho_path[k-1] # make rho to zero
rho_path[k] = rho_path[k-1] - delta_rho
beta_path[active_set, k] = beta_path[active_set, k-1, drop=F] - delta_rho * delta_b
objval_path[k] = sum((y - X %*% beta_path[ ,k])^2) + rho_path[k] * sum(abs(beta_path[k]))
lambda_patheq[, k] = lambda_patheq[, k-1] - delta_rho * delta_l
break
}
}
### updates
rho_path[k] = rho_path[k-1] - delta_rho
beta_path[active_set, k] = beta_path[active_set, k-1, drop=F] - delta_rho * delta_b
objval_path[k] = sum((y - X %*% beta_path[ ,k])^2) + rho_path[k] * sum(abs(beta_path[k]))
lambda_patheq[, k] = lambda_patheq[, k-1] - delta_rho * delta_l
# case 1) subgradient rule is violated
if(min_idx == 1) {
active_set[sub_viol] = F
# case 2) KKT condition is violated
} else if(min_idx == 2) {
active_set[which(!active_set)[kkt_viol]] = T
}
if(length(kkt_viol_to_active) > 0) {
active_set[which(!active_set)[kkt_viol_to_active]] = T
}
num_active = sum(active_set)
}
beta_path
k
k = 11
### find direction
H = t(X) %*% X
M = rbind(cbind(H[active_set, active_set], t(Aeq[, active_set])),
cbind(Aeq[, active_set], matrix(rep(0, m*m), nrow = m)))
S = rbind(subgrad[active_set, , drop=F], matrix(rep(0, m), ncol = 1))
if(min(eigen(M)$values) > 0) {
b_l = solve(M, S)
} else {
b_l = MASS::ginv(M) %*% S
}
# derivative for beta and lambda for rho
delta_b = b_l[1:num_active, ,drop=F]
delta_l = b_l[(num_active+1):nrow(b_l), ,drop=F]
### find delta_rho
delta_rho_vec = c()
# 1. active -> inactive (== subgradient rule check)
nonzero_active_beta = which(active_set)[which(beta_path[active_set, k-1] != 0)]
# predictor have potential to shrink 0
# -> which beta coef and delta_beta have different direction
sub_mismatch = sign(beta_path[nonzero_active_beta, k-1]) * sign(delta_b[which(beta_path[active_set, k-1] != 0), ])
any(sub_mismatch == 1)
delta_rho_sub_tmp = beta_path[nonzero_active_beta[which(sub_mismatch == 1)], k-1] /
delta_b[which(beta_path[active_set, k-1] != 0), ][which(sub_mismatch == 1)]
delta_rho = min(delta_rho_sub_tmp)
delta_rho_vec = c(delta_rho_vec, delta_rho)
# predictor shrink to zero
sub_viol = which(active_set)[which(beta_path[which(active_set), k-1] / delta_b == delta_rho)]
delta = Variable(1)
constraints = list(-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
<= (rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
>= -(rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
delta >= 0)
objective = Maximize(delta)
problem = Problem(objective, constraints)
delta = Variable(1)
constraints = list(-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
<= (rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
-t(X[, !active_set]) %*% (y - X[, active_set] %*% (beta_path[active_set, k-1] - delta * delta_b)) +
t(Aeq[, !active_set]) %*% (lambda_patheq[, k-1] - delta * delta_l)
>= -(rho_path[k-1] - delta) * matrix(rep(1, sum(!active_set)), ncol = 1),
delta >= 0)
objective = Maximize(delta)
sum(active_set) != p
delta_rho_vec = c(delta_rho_vec, -1)
min(delta_rho_vec[delta_rho_vec > 0]) > rho_path[k-1]
rho_path[k-1]
delta_rho_vec[1] > 0 & delta_rho_vec[2] < 0
delta_rho = delta_rho_vec[1]
min_idx = 1
### updates
rho_path[k] = rho_path[k-1] - delta_rho
beta_path[active_set, k] = beta_path[active_set, k-1, drop=F] - delta_rho * delta_b
objval_path[k] = sum((y - X %*% beta_path[ ,k])^2) + rho_path[k] * sum(abs(beta_path[k]))
lambda_patheq[, k] = lambda_patheq[, k-1] - delta_rho * delta_l
min_idx
active_set[sub_viol] = F
active_set
kkt_viol_to_active
if(length(kkt_viol_to_active) > 0) {
active_set[which(!active_set)[kkt_viol_to_active]] = T
}
active_set
num_active = sum(active_set)
num_active
k
k = 12
### find direction
H = t(X) %*% X
M = rbind(cbind(H[active_set, active_set], t(Aeq[, active_set])),
cbind(Aeq[, active_set], matrix(rep(0, m*m), nrow = m)))
S = rbind(subgrad[active_set, , drop=F], matrix(rep(0, m), ncol = 1))
if(min(eigen(M)$values) > 0) {
b_l = solve(M, S)
} else {
b_l = MASS::ginv(M) %*% S
}
# derivative for beta and lambda for rho
delta_b = b_l[1:num_active, ,drop=F]
delta_b
delta_l = b_l[(num_active+1):nrow(b_l), ,drop=F]
### find delta_rho
delta_rho_vec = c()
# 1. active -> inactive (== subgradient rule check)
nonzero_active_beta = which(active_set)[which(beta_path[active_set, k-1] != 0)]
# predictor have potential to shrink 0
# -> which beta coef and delta_beta have different direction
sub_mismatch = sign(beta_path[nonzero_active_beta, k-1]) * sign(delta_b[which(beta_path[active_set, k-1] != 0), ])
any(sub_mismatch == 1)
delta_rho_vec = c(delta_rho_vec, -1)
sum(active_set) != p
delta_rho_vec = c(delta_rho_vec, -1)
delta_rho_vec
any(delta_rho_vec > 0)
delta_rho_vec[1] < 0 & delta_rho_vec[2] < 0
length(kkt_viol_to_active) > 0
kkt_viol_to_active
kkt_viol_to_active = c()
length(kkt_viol_to_active) > 0
rm(list = ls())
gc()
#####################################
# JUST FOR ORDINARY REGRESSION LOSS #
#####################################
#
setwd("C:/Users/dpelt/OneDrive - 서울시립대학교/Documents/GitHub/ML_study/conlasso_eq/src")
source("conlasso_init.R")
source("conlasso_eq.R")
#
set.seed(520)
### setting --------------------------------------------------
# dimension
n = 100
p = 10
m = 5
# data
X = matrix(rnorm(n*p), nrow = n)
X.m = apply(X, 2, mean)
X.m = matrix(X.m, nrow = n, ncol = p, byrow = T)
X = X - X.m
true_b = rep(1, p)
y = X %*% true_b + rnorm(n)
y = y - mean(y)
n = dim(X)[1]
p = dim(X)[2]
### equality constraints
# Aeq = matrix(rnorm(m*p, 0, 1), nrow = m)
Aeq = matrix(runif(m*p, min = -1, max = 2), nrow = m)
beq = matrix(rep(0, m), nrow = m)
# penalty weight*****(NOT USED)
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
###
l = conlasso_eq(X, y, Aeq, beq, show_plotting=T)
l
rm(list = ls())
gc()
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#####################################
# JUST FOR ORDINARY REGRESSION LOSS #
#####################################
#
set.seed(520)
### setting --------------------------------------------------
# dimension
n = 100
p = 10
k = 5
# data
X = matrix(rnorm(n*p), nrow = n)
X.m = apply(X, 2, mean)
X.m = matrix(X.m, nrow = n, ncol = p, byrow = T)
X = X - X.m
true_b = rep(1, p)
y = X %*% true_b + rnorm(n)
y = y - mean(y)
n = dim(X)[1]
p = dim(X)[2]
### inequality constraints
# Aeq = matrix(rnorm(m*p, 0, 1), nrow = m)
Aineq = matrix(runif(k*p, min = -1, max = 2), nrow = k)
bineq = matrix(rep(0, k), nrow = k)
# penalty weight*****(NOT USED)
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
n = dim(X)[1]
p = dim(X)[2]
k = dim(Aineq)[1]
# setting
zero_p = matrix(rep(0, p), ncol = 1)
zero_k = matrix(rep(0, k), ncol = 1)
zero_pk = matrix(rep(0, p*k), nrow = p)
zero_kp = matrix(rep(0, p*k), nrow = k)
zero_pp = matrix(rep(0, p*p), nrow = p)
one_p = matrix(rep(1, p), ncol = 1)
one_k = matrix(rep(1, k), ncol = 1)
I_p = matrix(diag(rep(1, p)), nrow = p)
I_k = matrix(diag(rep(1, k)), nrow = k)
# design matrix for Constraints
D1 = matrix(c(1, t(zero_p), t(zero_k)), nrow = 1)
D2 = rbind(c(0, t(zero_p), t(zero_k)),
cbind(zero_p, I_p, zero_pk),
c(0, t(zero_p), t(zero_k)))
D3 = rbind(c(0, t(zero_p), t(zero_k)),
cbind(zero_p, zero_pp, t(Aineq)),
c(0, t(zero_p), t(zero_k)))
D4 = rbind(0, t(X) %*% y, 0)
D5 = rbind(c(0, t(zero_p), t(zero_k)),
cbind(one_p, zero_pp, zero_pk),
c(0, t(zero_p), t(zero_k)))
D6 = cbind(zero_k, zero_kp, I_k)
# solve
target = Variable(1 + p + k)
constraints = list(D2 %*% target == D3 %*% target,
D2 %*% target <= D4 + D5 %*% target,
D2 %*% target >= D4 - D5 %*% target,
D1 %*% target >= 0,
D6 %*% target >= zero_k)
objective = Minimize(D1 %*% target)
problem = Problem(objective, constraints)
result = solve(problem)
# result
target = result$getValue(target)
rho_max = target[1, ]
# z = target[2:(1+p), ,drop=F]
nu_max = target[(p+2):nrow(target), ,drop=F]
# violation
idx1 = which(abs(-t(X) %*% y + rho_max * one_p + t(Aineq) %*% nu_max) <= 1e-4)
idx2 = which(abs(-t(X) %*% y - rho_max * one_p + t(Aineq) %*% nu_max) <= 1e-4)
activeset = sort(union(idx1, idx2))
# full rank check
flag = T
if(dim(Aeq)[1] > 1) flag = Matrix::rankMatrix(t(Aineq)[activeset, ]) == ncol(t(Aineq)[activeset, ])
rm(list = ls())
gc()
if(!require("CVXR")) install.packages("CVXR")
library("CVXR")
#####################################
# JUST FOR ORDINARY REGRESSION LOSS #
#####################################
#
set.seed(520)
### setting --------------------------------------------------
# dimension
n = 100
p = 10
k = 5
# data
X = matrix(rnorm(n*p), nrow = n)
X.m = apply(X, 2, mean)
X.m = matrix(X.m, nrow = n, ncol = p, byrow = T)
X = X - X.m
true_b = rep(1, p)
y = X %*% true_b + rnorm(n)
y = y - mean(y)
n = dim(X)[1]
p = dim(X)[2]
### inequality constraints
# Aeq = matrix(rnorm(m*p, 0, 1), nrow = m)
Aineq = matrix(runif(k*p, min = -1, max = 2), nrow = k)
bineq = matrix(rep(0, k), nrow = k)
# penalty weight*****(NOT USED)
penwt = rep(1, p) # 20-element Array{Frhoat64,1}
n = dim(X)[1]
p = dim(X)[2]
k = dim(Aineq)[1]
# setting
zero_p = matrix(rep(0, p), ncol = 1)
zero_k = matrix(rep(0, k), ncol = 1)
zero_pk = matrix(rep(0, p*k), nrow = p)
zero_kp = matrix(rep(0, p*k), nrow = k)
zero_pp = matrix(rep(0, p*p), nrow = p)
one_p = matrix(rep(1, p), ncol = 1)
one_k = matrix(rep(1, k), ncol = 1)
I_p = matrix(diag(rep(1, p)), nrow = p)
I_k = matrix(diag(rep(1, k)), nrow = k)
# design matrix for Constraints
D1 = matrix(c(1, t(zero_p), t(zero_k)), nrow = 1)
D2 = rbind(c(0, t(zero_p), t(zero_k)),
cbind(zero_p, I_p, zero_pk),
c(0, t(zero_p), t(zero_k)))
D3 = rbind(c(0, t(zero_p), t(zero_k)),
cbind(zero_p, zero_pp, t(Aineq)),
c(0, t(zero_p), t(zero_k)))
D4 = rbind(0, t(X) %*% y, 0)
D5 = rbind(c(0, t(zero_p), t(zero_k)),
cbind(one_p, zero_pp, zero_pk),
c(0, t(zero_p), t(zero_k)))
D6 = cbind(zero_k, zero_kp, I_k)
# solve
target = Variable(1 + p + k)
constraints = list(D2 %*% target == D3 %*% target,
D2 %*% target <= D4 + D5 %*% target,
D2 %*% target >= D4 - D5 %*% target,
D1 %*% target >= 0,
D6 %*% target >= zero_k)
objective = Minimize(D1 %*% target)
problem = Problem(objective, constraints)
result = solve(problem)
# result
target = result$getValue(target)
rho_max = target[1, ]
# z = target[2:(1+p), ,drop=F]
nu_max = target[(p+2):nrow(target), ,drop=F]
# violation
idx1 = which(abs(-t(X) %*% y + rho_max * one_p + t(Aineq) %*% nu_max) <= 1e-4)
idx2 = which(abs(-t(X) %*% y - rho_max * one_p + t(Aineq) %*% nu_max) <= 1e-4)
activeset = sort(union(idx1, idx2))
# full rank check
flag = T
if(dim(Aineq)[1] > 1) flag = Matrix::rankMatrix(t(Aineq)[activeset, ]) == ncol(t(Aineq)[activeset, ])
nu_max
-t(X) %*% y + rho_max * one_p + t(Aineq) %*% nu_max
Matrix::rankMatrix(t(Aineq)[activeset, ])
ncol(t(Aineq)[activeset, ])
