\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{natbib}
\usepackage{multirow}
\usepackage[svgnames]{xcolor}
\usepackage{listings}
\usepackage{kotex}
\usepackage{float}
\usepackage[algoruled,boxed,lined]{algorithm2e}


\def\cA{{\cal A}}
\def\cB{{\cal B}}
\def\cC{{\cal C}}
\def\cD{{\cal D}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\cK{{\cal K}}
\def\cL{{\cal L}}
\def\cN{{\cal N}}
\def\cG{{\cal G}}
\def\cX{{\cal X}}
\def\cY{{\cal Y}}
\def\cP{{\cal P}}
\def\cS{{\cal S}}
\def\cI{{\cal I}}
\def\cU{{\cal U}}

\def\qed{\space$\square$ \par \vspace{.15in}}

\newcommand{\bA}{{\bf A}}
\newcommand{\bD}{{\bf D}}
\newcommand{\bC}{{\bf C}}
\newcommand{\bT}{{\bf T}}
\newcommand{\bt}{{\bf t}}
\newcommand{\bZ}{{\bf Z}}
\newcommand{\by}{{\bf y}}
\newcommand{\bY}{{\bf Y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bN}{{\bf N}}
\newcommand{\bn}{{\bf n}}
\newcommand{\bfE}{{\bf E}}
\newcommand{\bfe}{{\bf e}}
\newcommand{\bg}{{\bf g}}
\newcommand{\bff}{{\bf f}}
\newcommand{\bF}{{\bf F}}
\newcommand{\uxj}{x^{(j)}}
\newcommand{\uxk}{x^{(k)}}
\newcommand{\uf}{{\underline f}}
\newcommand{\uH}{{\underline H}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bW}{{\bf W}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bV}{{\bf V}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bU}{{\bf U}}
\newcommand{\bI}{{\bf I}}
\newcommand{\bH}{{\bf H}}
\newcommand{\bh}{{\bf h}}
\newcommand{\bfs}{{\bf s}}
\newcommand{\bfa}{{\bf a}}
\newcommand{\hba}{\hat{\bf a}}
\newcommand{\blXn}{{\bf \underline{X_n}}}
\newcommand{\E}{\mbox{{\rm E}}}
\newcommand{\V}{\mbox{{\rm Var}}}
\newcommand{\ri}{\mbox{{\bf \rm ri}}}
\newcommand{\co}{\mbox{{\rm co}}}
\newcommand{\lspan}{\mbox{{\rm lspan}}}
\newcommand{\hbeta}{\hat{\beta}}
\newcommand{\btheta}{\mbox{\boldmath{$\theta$}}}
\newcommand{\hbtheta}{\mbox{\boldmath{$\hat{\theta}$}}}
\newcommand{\balpha}{\mbox{\boldmath{$\alpha$}}}
\newcommand{\hbalpha}{\mbox{\boldmath{$\hat{\alpha}$}}}
\newcommand{\bepsilon}{\mbox{\boldmath{$\epsilon$}}}
\newcommand{\bbeta}{\mbox{\boldmath{$\beta$}}}
\newcommand{\bdelta}{\mbox{\boldmath{$\delta$}}}
\newcommand{\bgamma}{\mbox{\boldmath{$\gamma$}}}
\newcommand{\blambda}{\mbox{\boldmath{$\lambda$}}}
\newcommand{\bLambda}{\mbox{\boldmath{$\Lambda$}}}
\newcommand{\bfeta}{\mbox{\boldmath{$\eta$}}}
\newcommand{\bSigma}{\mbox{\boldmath{$\Sigma$}}}
\newcommand{\bmu}{\mbox{\boldmath{$\mu$}}}
\newcommand{\bTheta}{\mbox{\boldmath{$\Theta$}}}
\newcommand{\bphi}{\mbox{\boldmath{$\phi$}}}
\newcommand{\bleta}{\mbox{\boldmath{$\eta$}}}
\newcommand{\bzero}     {\mbox{\boldmath{$0$}\unboldmath}}

\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newtheorem{example}{\bf Example}
\newtheorem{definition}{\bf Defintion}
\newtheorem{lemma}{\bf Lemma}
\newtheorem{theorem}{\bf Theorem}
\newtheorem{corollary}{\bf Corollary}
\newtheorem{proposition}{\bf Proposition}
\newtheorem{remark}{\bf Remark}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\bed}{\begin{itemize}}
\newcommand{\eed}{\end{itemize}}
\newcommand{\vs}{\vspace}


\newcommand{\mbR}{\mathbb{R}}
\newcommand{\mbQ}{\mathbb{Q}}
\newcommand{\mbB}{\mathbb{B}}

\newcommand{\sbmu}       {\mbox{\scriptsize\boldmath{$\mu$}\unboldmath}}
\newcommand{\sbphi}       {\mbox{\scriptsize\boldmath{$\phi$}\unboldmath}}
\newcommand{\sbtheta}       {\mbox{\scriptsize\boldmath{$\theta$}\unboldmath}}
\newcommand{\sbbeta}       {\mbox{\scriptsize\boldmath{$\beta$}\unboldmath}}
\newcommand{\sbalpha}       {\mbox{\scriptsize\boldmath{$\alpha$}\unboldmath}}
\newcommand{\sbgamma}       {\mbox{\scriptsize\boldmath{$\gamma$}\unboldmath}}

\newcommand{\one}{^{(1)}}
\usepackage[utf8]{inputenc}

\title{Initialization of Equality Constrained LASSO}
\author{안승환}
\date{2019}

\begin{document}

\maketitle

\tableofcontents

\section{Problem}
Constrained lasso problem with only equality constraints: 
\bea
\begin{aligned}
    &\text{minimize} && L(\bbeta) + \rho ||\bbeta||_1 \\
    &\text{subject to } && \bA \bbeta = 0 
\end{aligned}
\eea
(In this paper, we would think of $L(\bbeta)$ as $\frac{1}{2} ||\by - \bX \bbeta||_2^2$.)

Since we perform path following in the decreasing direction of penalty parameter $\rho$, an initializing value for the parameter $\rho$ is needed. ($\by \in \mathbb{R}^n, \bX \in \mathbb{R}^{n \times p}, \bbeta \in \mathbb{R}^p, \bA \in \mathbb{R}^{m \times p}$, and $m$ is the number of constraints.)

\bigskip
As $\rho$ \rightarrow \: $\infty$, the solution $\bbeta$ to the original problem is given by 
\bea
\begin{aligned}
    &\text{minimize} && ||\bbeta||_1 \\
    &\text{subject to } && \bA \bbeta = 0 
\end{aligned}
\eea
And obviously, the solution $\hat \bbeta$ for the above problem is $0_p$.

\section{Initialization}
The stationarity of KKT conditions for (2) is as follows:
$$\nabla L(\bbeta) + \rho sign(\bbeta) + \bA^T \lambda = 0_p$$,
where $\lambda \in \mathbb{R}^m$ is lagrangian multiplier and $sign(\bbeta)$ is the subgradient of $||\bbeta||_1$.
Since $|sign(\bbeta)| \leq 1_p$, we can transform above stationarity condition as follows:
$$\lvert \nabla L(\bbeta) + \bA^T \lambda \rvert \leq \rho 1_p$$

\begin{lemma}
For fixed $\rho$, $\bbeta$, let $\mathcal{E}_{\rho}(\bbeta) = \{\lambda \in \mathbb{R}^m : \rvert \nabla L(\bbeta) + \bA^T \lambda \rvert \leq \rho 1_p\}$, and let $\rho_{max} = inf\{\rho \in \mathbb{R} : \mathcal{E}_{\rho}(\bbeta) \neq \varnothing \}$.
\end{lemma}

Then for $\rho < \rho_{max}$, $\bbeta = 0_p$ is not solution of (1). (This can be proved by Seperating Hyperplane Theorem?)

\begin{corollary}
The minimizer of (1) for a $\rho$ is $\bbeta = 0_p$ if and only if $\rho \geq \rho_{max}$, where $\rho_{max}$ is the optimal solution of (3).
And also, we can get the solution $\lambda_{max}$ corresponding to $\rho_{max}$ by (3).
\end{corollary}

\bea
\begin{aligned}
    &\text{minimize} && \rho \\
    &\text{subject to } && z = \bA^T \lambda \\
    & && z \leq -\nabla L(\bbeta) + \rho 1 \\
    & && z \geq -\nabla L(\bbeta) - \rho 1 \\
    & && \rho \geq 0
\end{aligned}
\eea

\textbf{Active Set} \\
So, we can initialize active set $\mathcal{A}$ as follows:
$$\mathcal{A} = \{j : \lvert \nabla L(\bbeta)_j + a_j^T \lambda_{max} \rvert = \rho_{max}\}$$
where $\bA = [a_1, \cdots a_p]$.

If we decrease $\rho$ very little as $\rho < \rho_{max}$, $\hat \beta_j = 0$ cannot be the coefficient of optimal solution (1) for predictor $x_j$.
So, predictor $x_j$ must be activated as $\rho$ decreasing.

\bigskip
\textbf{Uniqueness of $\lambda_{max}$} \\
$\lambda_{max}$ is the unique solution to (3) if the solution for $\bA_{\mathcal{A}}^T \Tilde{\lambda} = 0$ is only $\Tilde{\lambda} = 0$. ($\Leftrightarrow \bA_{\mathcal{A}}^T$ is full column).

Because we can formulate an equation for predictors which are set on boundaries of the stationarity condition(which compose active set $\mathcal{A}$) as follows:
$$\lvert \nabla L(\bbeta)_{\mathcal{A}} + \bA_{\mathcal{A}}^T (\lambda_{max} + \Tilde{\lambda}) \rvert = \rho_{max} 1_{\mathcal{A}}$$

\section{Completeness of Active Set}
In this section, our main question is : What happens if we do NOT activate all the violated predictors of $\mathcal{A}$?

\bigskip
Let $L(\bbeta)$ = $\frac{1}{2} ||\by - \bX \bbeta||_2^2$. Given $\rho_{max}, \lambda_{max}$, we want $\mathcal{B} (\subseteq \mathcal{A}$), $\Delta \rho, \frac{d}{d\rho}\bbeta_{\mathcal{B}}$, and $\frac{d}{d\rho}\lambda_{\mathcal{B}}$ such that satisfy following conditions(stationarity from KKT conditions, and the equality constraint):
\bea
-\bX_{:\mathcal{B}}^T (\by - \bX_{:\mathcal{B}}(\bbeta_{\mathcal{B}}^{(0)} - \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}})) &+& \nonumber \\
(\rho_{max} - \Delta \rho) sign(\bbeta_{\mathcal{B}}^{(0)} - \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}}) + \bA_{:\mathcal{B}}^T (\lambda_{max} - \Delta \rho \frac{d}{d\rho}\lambda) &=& 0 \\
\lvert \bX_{:\mathcal{B^C}}^T (\by - \bX_{:\mathcal{B}}(\bbeta_{\mathcal{B}}^{(0)} - \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}})) - \bA_{:\mathcal{B^C}}^T (\lambda_{max} - \Delta \rho \frac{d}{d\rho}\lambda) \rvert &\leq& \nonumber \\
(\rho_{max} - \Delta \rho) 1_{|\mathcal{B^C}|}&& \\ 
\bA_{:\mathcal{B}}(\bbeta_{\mathcal{B}}^{(0)} - \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}}) &=& 0 
\eea

$\rho$ is in decreasing direction, so $\Delta \rho > 0$. 
And the moving direction of $\bbeta$ is must be maintained.
So, $$sign(\bbeta_{\mathcal{B}}^{(0)} - \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}}) = sign(\bbeta^{(0)})$$.
From \textbf{Corollary 1}, we have following:
\bea
-\bX_{:\mathcal{B}}^T (\by - \bX_{:\mathcal{B}}\bbeta_{\mathcal{B}}^{(0)}) + \rho_{max} sign(\bbeta_{\mathcal{B}}^{(0)}) + \bA_{:\mathcal{B}}^T (\lambda_{max}) &=& 0 \\
\lvert \bX_{:\mathcal{B^C}}^T (\by - \bX_{:\mathcal{B}}\bbeta_{\mathcal{B}}^{(0)}) - \bA_{:\mathcal{B^C}}^T \lambda_{max} \rvert &\leq& \rho_{max} 1_{|\mathcal{B^C}|} \\
\bA_{:\mathcal{B}}\bbeta_{\mathcal{B}}^{(0)} &=& 0 
\eea

Finally, we get
\bea
\bX_{:\mathcal{B}}^T\bX_{:\mathcal{B}} \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}} - \Delta \rho sign(\bbeta_{\mathcal{B}}^{(0)}) + \bA_{:\mathcal{B}}^T \Delta \rho \frac{d}{d\rho}\lambda &=& 0 \\
 \bA_{:\mathcal{B}} \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}} &=& 0 \\
\lvert \bX_{:\mathcal{B^C}}^T \bX_{:\mathcal{B}} \Delta \rho \frac{d}{d\rho}\bbeta_{\mathcal{B}} + \bA_{:\mathcal{B^C}}^T \Delta \rho \frac{d}{d\rho}\lambda \rvert &\leq& \Delta \rho 1_{|\mathcal{B^C}|} 
\eea

Let's focus on first two equations (10), (11):
\bea
\begin{bmatrix} 
\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}} & \bA_{:\mathcal{B}}^T  \\
\bA_{:\mathcal{B}} & 0
\end{bmatrix}
\begin{bmatrix}
\frac{d}{d\rho}\bbeta_{\mathcal{B}} \\
\frac{d}{d\rho}\lambda
\end{bmatrix}
&=& 
\begin{bmatrix}
sign(\bbeta_{\mathcal{B}}^{(0)}) \\
0
\end{bmatrix} 
\eea
And, 
\bea
\begin{bmatrix}
\frac{d}{d\rho}\bbeta_{\mathcal{B}} \\
\frac{d}{d\rho}\lambda
\end{bmatrix}
=
\begin{bmatrix} 
\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}} & \bA_{:\mathcal{B}}^T  \\
\bA_{:\mathcal{B}} & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
sign(\bbeta_{\mathcal{B}}^{(0)}) \\
0
\end{bmatrix} 
\eea

\bigskip
\textbf{Assumption 1}
Rows of $\bA_{:\mathcal{B}}$ are linearly independent. 

\bigskip
So,
$\begin{bmatrix} 
\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}} & \bA_{:\mathcal{B}}^T  \\
\bA_{:\mathcal{B}} & 0
\end{bmatrix}$
is invertible by \textbf{Assumption 1}.

\medskip
Obviously, $\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}}$ is invertible.
By inverse of block matrix, we have following equation:
\bea
\begin{bmatrix}
\frac{d}{d\rho}\bbeta_{\mathcal{B}} \\
\frac{d}{d\rho}\lambda
\end{bmatrix} 
= \nonumber \\
\begin{bmatrix}
(\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} - (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T Z^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} & (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T Z^{-1} \\
Z^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} & - Z^{-1} \\
\end{bmatrix} 
\begin{bmatrix}
sign(\bbeta_{\mathcal{B}}^{(0)}) \\
0
\end{bmatrix} 
\eea
where $Z = \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T$ and here, inverse of $Z$($Z^{-1}$) could be not only its inverse but also its generalized inverse.

Therefore,
\bea
\frac{d}{d\rho}\bbeta_{\mathcal{B}} &=& \left( (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} - (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T Z^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \right) sign(\bbeta_{\mathcal{B}}^{(0)}) \nonumber \\
&=& \left( (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1}(\bI - \bW) \right) sign(\bbeta_{\mathcal{B}}^{(0)})
\eea,

where $$\bW = \bA_{:\mathcal{B}}^T Z^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} = \bA_{:\mathcal{B}}^T (\bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T)^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1}$$.

If $\frac{d}{d\rho}\bbeta_{\mathcal{B}} \approx 0$, for new active set $\mathcal{B}$, there is no direction to move $\bbeta_{\mathcal{B}}$ that satisfies KKT conditions. \\
If not, we check $\frac{d}{d\rho}\bbeta_{\mathcal{B}}$ and $\frac{d}{d\rho}\lambda$ for following condition:
\bea
\lvert \bX_{:\mathcal{B^C}}^T \bX_{:\mathcal{B}} \frac{d}{d\rho}\bbeta_{\mathcal{B}} + \bA_{:\mathcal{B^C}}^T \frac{d}{d\rho}\lambda \rvert &\leq& 1_{|\mathcal{B^C}|}
\eea
(And it can be easily shown that for all $\mathcal{B}^C$, above inequality is satisfied). 

\bigskip
(Trivial: If $\bA_{:\mathcal{B}}$ is invertible, 
\bea
\begin{bmatrix} 
\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}} & \bA_{:\mathcal{B}}^T  \\
\bA_{:\mathcal{B}} & 0
\end{bmatrix}^{-1}
=
\begin{bmatrix} 
0 & \bA_{:\mathcal{B}}^{-1}  \\
(\bA_{:\mathcal{B}}^T)^{-1} & - (\bA_{:\mathcal{B}}^T)^{-1} \bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}} \bA_{:\mathcal{B}}^{-1} 
\end{bmatrix}.
\eea
So, $\frac{d}{d\rho}\bbeta_{\mathcal{B}} = 0$ and there is no direction to move.)

\subsection{Where $\mathcal{B} = \mathcal{A}$ ($|\mathcal{B}| = m + 1$)}
(See Appendix 2)
\bed
    \item $\bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T$ \textbf{is invertible.} \\
    So, $\bA_{:\mathcal{B}}$ is right-invertible, and $\bA_{:\mathcal{B}}^T$ is left-invertible.
    And let $\bA_{:\mathcal{B}} \bA_{:\mathcal{B}}^{-1} = \bI$, and $\bA_{:\mathcal{B}}^{-T} \bA_{:\mathcal{B}}^{T} = \bI$ with their right and left inverses.
    \bean
    \bW &=& \bA_{:\mathcal{B}}^T \bA_{:\mathcal{B}}^{-T} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}}) \bA_{:\mathcal{B}}^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \\
    &\neq& \bI
    \eean
    Because the order of left and right inverse matrix product is changed.
    So, $\frac{d}{d\rho}\bbeta_{\mathcal{B}} \neq 0$.\\
\eed

\subsection{Where $|\mathcal{B}| = m$}
\bed
    \item \textbf{$\bA_{:\mathcal{B}}$ is invertible (by Assumption 1)} \\
    \bean
    \bW &=& \bA_{:\mathcal{B}}^T \bA_{:\mathcal{B}}^{-T} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}}) \bA_{:\mathcal{B}}^{-1} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \\
    &=& \bI
    \eean
    Therefore, $\frac{d}{d\rho}\bbeta_{\mathcal{B}} = 0$.\\
\eed

\subsection{Where $|\mathcal{B}| < m$}
\bed
    \item \textbf{$\bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T$ is NOT invertible} (See Appendix 1) \\
    So, we use generalized inverse. \\
    (\textbf{Assumption 2} Columns of $\bA_{:\mathcal{B}}$ are linearly independent.)
    \bean
    \bW &=& \bA_{:\mathcal{B}}^T (\bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \bA_{:\mathcal{B}}^T)^{-} \bA_{:\mathcal{B}} (\bX_{:\mathcal{B}}^T \bX_{:\mathcal{B}})^{-1} \\
    &=& \bI
    \eean
    where $\bA^{-}$ means generalized inverse of matrix $\bA$.
    (This can be proved using the definition of generalized inverse and \textbf{Assumption 2}). \\
    Therefore, $\frac{d}{d\rho}\bbeta_{\mathcal{B}} = 0$.
\eed

\section{Appendix}
\subsection{Appendix 1}
\textbf{For $H \in \mathbb{R}^{m \times n}$, $H H^T$ is not invertible, where $m > n$}

\medskip
proof) \\
The columns of $H^T$ are linearly dependent. 
So, there exists $x \neq 0$ such that $H^T x = 0$.
$H H^T x = 0$ and this means 0 is an eigenvalue of $H H^T$.
Therefore, $\lvert H H^T \rvert = 0$ and $H H^T$ is singular.
(The determinant of matrix is the product of eigenvalues of the matrix).

\subsection{Appendix 2}
\textbf{Why $|\mathcal{A}|$ is always $m + 1$?} 

\medskip
For setting initial active set, we solve following problem and find predictors which set on the boundary of inequalities. And these predictors are chose for initial active set.
\bea
\begin{aligned}
    &\text{minimize} && \rho \\
    &\text{subject to } && z = \bA^T \lambda \\
    & && z \leq -\nabla L(\bbeta) + \rho 1 \\
    & && z \geq -\nabla L(\bbeta) - \rho 1 \\
    & && \rho \geq 0
\end{aligned}
\eea

And we can change above problem to the problem having same purpose (getting initial active set) as following: 

\medskip
Find $\lambda$, and minimum $\rho$ satisfying
\bean
\begin{bmatrix}
\bA_{\mathcal{A}}^T & \pm 1_{|\mathcal{A}|}
\end{bmatrix}
\begin{bmatrix}
\lambda \\
\rho
\end{bmatrix}
=
\begin{bmatrix}
- \bX_{\mathcal{A}}^T y
\end{bmatrix}
\eean.
And predictors of $\mathcal{A}^C$ are set between lower and upper bounds of inequalities.
The unknown variable $\begin{bmatrix} \lambda \\ \rho \end{bmatrix}$ is $m + 1$ dimension.
So, when $|\mathcal{A}| = m + 1$, this linear programming has unique solution.

\medskip
(Application of this property: If you want to activate only $q$ predictors at the initialization step, then you should set $q-1$ constraints (it means $A \in \mathbb{R}^{q-1 \times p}$).

\end{document}